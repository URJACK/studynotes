# Classification

## 引入

### 线性回归

如果使用回归的方式来做分类：

我们对二维的参数，尝试拟合一条直线。处在这条直线上的点，函数值应该为0。

显然，任何一组二维输入[x1,x2]，都会存在大于0，小于0的情况，我们将大于0的归为一类，小于0的归为一类。



但用一条直线来进行分类，显然存在的一个问题是，如果有一些样本数的数值过大的话，这些过大的数值会将拟合的直线给“拉偏”，从而使得最终的结果达不到预期。

![image-20200903093123025](.\classification\image-20200903093123025.png)

### 替代方案

一个替代方案是，设置一个函数**f(x)**：

```
g(x) > 0归为class1
g(x) <= 0归为class2
```

使用该函数，我们对每一个样本进行预测，只要分类不同，就进行一次计数
$$
样本x^i\\
f(x^i)\neq y^i
$$
统计最终的个数。就是损失函数。

![image-20200903093420357](.\classification\image-20200903093420357.png)

## 盒子问题

### 引入

这里使用盒子问题作为分类方法的引子

事件A=“抓出的蓝球，来自B1这个盒子。”。

如果我们需要求的问题是，事件A的概率。

那么计算方式如下：

![image-20200903093604989](.\classification\image-20200903093604989.png)

### 类比分类问题

那么这个盒子的模型，可以类比到分类问题中去。

事件B=“输入的x，属于C1这个分类”

求事件B的概率。

那么计算方式如下：

![image-20200903093859093](.\classification\image-20200903093859093.png)

### 公式中的P(C1),P(C2)

显而易见，可以等价的列写出类似事件的概率公式，比如“输入的x，属于C2这个分类”的概率公式。

现在我们需要解决的是，解决公式中一些变量的来源

首先P(C1),P(C2)这两个数值是非常容易求得的，我们只需统计训练集中的对应分类的个数，计算他们的百分比，就可以求得这两个数值了。

![image-20200903094304390](.\classification\image-20200903094304390.png)

### 公式中的P(x|c1),P(x|c2)

还差P(x|c1),P(x|C2)这两个家伙无法求得。他们两个的含义分别是：

c1这个分类中，能取到x的概率是多少。

c2这个分类中，能取到x的概率是多少。



![image-20200903094940771](.\classification\image-20200903094940771.png)

#### 高斯分布

为了计算这两个条件概率，需要引入高斯分布。

高斯分布包含了μ和Σ两个参数。μ的维度与x的维度相等（假设为n），不难推出Σ的维度是(n , n)。
$$
f_{μ,Σ}(x) = ....
$$
我们将输入代入上述表达式，就可以得到该x，在当前分布下的一个概率值。也即P(x|c1)

当然，每一个不同的分类，就有一个不同的高斯分布。

![image-20200903095440780](.\classification\image-20200903095440780.png)

当然，为了优化每一种类别的高斯分布，以优化A类别为例，假设A类别的所有样本个数有79个，那么为了优化这个高斯分布，我们需要将Σ和μ参数取按如下公式取值。

![image-20200903095730459](.\classification\image-20200903095730459.png)

尝试计算这个结果，就可以得到Σ和μ参数。

![image-20200903095821967](.\classification\image-20200903095821967.png)

#### 求得条件概率

有了Σ和μ参数，条件概率就可以求得了

代入计算就可以求出P(c1|x)

![image-20200903100108157](.\classification\image-20200903100108157.png)

绘制出分布情况，我们发现的话两个特征其实对于整体预测准确率并不高。（显然，可能**防御力**和**SP防御力**这两个数值与宝可梦的**种族**这个属性并没有太多的关系）

即便放入7个特征（准确率依然只有54%）准确度依然不高。

![image-20200903100444032](.\classification\image-20200903100444032.png)

### 优化方案

#### 共用Σ

对之前的模型进行优化，我们将两个高斯分布**共用同一个Σ**，当然μ还是使用各自的μ。

共用的Σ取值公式如下图所示。

![image-20200903155840674](.\classification\image-20200903155840674.png)

我们发现共用了一个Σ后，图像的边界变为了一条直线（在后文经过数学推导后，会理解为什么是一条直线）

使用共用Σ后，7个维度的分类过程中，准确率从54%升到了73%，这说明共用Σ是有效的。

![image-20200903160313008](.\classification\image-20200903160313008.png)

这种各个输入维度的数据互相之间如果是毫无联系的，可以称作朴素贝叶斯分类。

![image-20200903161722343](.\classification\image-20200903161722343.png)

#### 公式推导

经过公式推导，我们可以得出
$$
P(C1|x) = \sigma(wx + b)
$$
以下是推导过程

![image-20200903174000029](.\classification\image-20200903174000029.png)

![image-20200903174240944](.\classification\image-20200903174240944.png)

![image-20200903174333222](.\classification\image-20200903174333222.png)

![image-20200903174528604](.\classification\image-20200903174528604.png)

多个高斯分布使用相同的Σ

![image-20200903174914466](.\classification\image-20200903174914466.png)