# 台大课程3-深度学习预备

## 神经网络

### 神经网络的构造

一个神经网络由若干个神经元拼接而成。

一个神经元可以看作是一次回归（激活函数未必选用sigmoid这个值域为-1 到 1 的激活函数）

![image-20200907154410844](.\deeplearning_pre\image-20200907154410844.png)

全连接层，这意味着前一层的每一个神经元都会与后一层的每一个神经元相相连接，所以称为全连接层。

![image-20200907154733188](.\deeplearning_pre\image-20200907154733188.png)

深层网络，可以看出网络层级的加深，对最终的结果效果也越明显。

![image-20200907155315298](.\deeplearning_pre\image-20200907155315298.png)

### 神经网络的运算

神经网络的实际运算其实是矩阵运算。黄色的就是参数w的矩阵，绿色的就是bias的矩阵。

![image-20200907155526308](.\deeplearning_pre\image-20200907155526308.png)

### 神经网络的例子

例如一张图片看作是一个输入的话，它经过神经网络（一个拟合后的函数集合），就可以得到一个输出。

在这个图片中，输出是经过softmax后的十个维度的输出。

![image-20200907160214492](.\deeplearning_pre\image-20200907160214492.png)

至此，不再需要人工的手动的去分离出初始数据的相关特征，不再需要人去尝试寻找这个“函数集合”。

所有的工作就可以交给神经网络来做。那么神经网络应该具体设计多少层，每层又该有多少个神经元呢？

这个需要尝试与试错以及人的直觉...



那么有没有网路可以自己设计自己的网络结构呢？答案是有的...

在卷积神经网络中，并不是所有的神经元都与下一个神经元完全相连，它们是使用一种卷积运算的特殊连接方式。

![image-20200907162249523](.\deeplearning_pre\image-20200907162249523.png)

### 神经网络的损失函数

在该例中，我们可以看出神经网络的损失函数的（分类问题）与之前的分类问题几乎相同，都使用了交叉熵做损失函数。

![image-20200907165747176](.\deeplearning_pre\image-20200907165747176.png)

将批次中每一个样本的损失相加取平均，就可以得到批次的损失函数。

![image-20200907165839846](.\deeplearning_pre\image-20200907165839846.png)

### 神经网络与梯度

优化损失的方法依然是梯度下降法。

![image-20200907165949230](.\deeplearning_pre\image-20200907165949230.png)



显然，多层的神经网络....要求出每一个参数的梯度显然是很麻烦的（即便只有一层也相当的麻烦了）

这里可以使用到诸多的深度学习框架，来帮助解决求取梯度的问题。

![image-20200907170135656](.\deeplearning_pre\image-20200907170135656.png)

### 为什么神经网络需要“深”而不是“胖”

![image-20200907170409145](.\deeplearning_pre\image-20200907170409145.png)

从结果上来说，深层的神经网络，最终得到的效果会更好。

![image-20200910114845017](.\deeplearning_pre\image-20200910114845017.png)

一个比较直观的原因是：神经网络的前一层，可以抽离出一些特征，从而为后一层的特征提取带来积极影响。

![image-20200910151309362](.\deeplearning_pre\image-20200910151309362.png)

尽管只要参数够多，单层的网络就可以拟合任何函数，但是找到这个函数是非常缺少效率的。

下面是一个拟合函数的例子。发现，同样参数个数的情况下，“深”比“胖”更具有优势。

![image-20200910160444015](.\deeplearning_pre\image-20200910160444015.png)

尽管最开始的时候，数据的分布（高维数据映射到2维）没有被拉开，但越到后面的层，它们的分布就会拉的更开。

![image-20200910161759576](.\deeplearning_pre\image-20200910161759576.png)

为了如下的问题，必须使用更深的神经网络

![image-20200910161439980](.\deeplearning_pre\image-20200910161439980.png)

## 引入反向传播

### 链式法则

链式法则规定了多元微分的求导法则。

![image-20200908100519988](.\deeplearning_pre\image-20200908100519988.png)

为了引入反向传播，我们先观察一个单独的神经元

![image-20200908100825073](.\deeplearning_pre\image-20200908100825073.png)

在该神经元中，如果我们要求 **损失函数C对参数w**的微分

按照链式法则：

我们可以求**神经元输出z对参数w**的微分，再求**损失函数C对输出z**的微分

![image-20200908101012505](.\deeplearning_pre\image-20200908101012505.png)

### 前项传播

其中z对w的微分较为好求，称作前项传播，就刚好等于输入的x的数值。

多层神经网络中，每一个神经网络的前项传播都是与对应输入相等的。

![image-20200908101341151](.\deeplearning_pre\image-20200908101341151.png)

### 反向传播

**损失函数C对输出z**这个反向传播的部分就稍微麻烦点了。

在这里，为了引入这个求法。必须再次应用到链式法则。



这里对微分应用链式法则拆解后，可以得到两个部分：

1·a对z求微分--即激活函数的导数

2·C对a求微分--（反向传播的来源）

![image-20200908101840880](.\deeplearning_pre\image-20200908101840880.png)

每一层的神经元的反向传播部分，都依赖于下一层神经元的反向传播部分

具体的计算公式整理如下：

![image-20200908102141211](.\deeplearning_pre\image-20200908102141211.png)

最后一层的方向传播我们是可以计算出来的，因为损失函数定义中的自变量，就是最后一层的输出z。

![image-20200908102904767](.\deeplearning_pre\image-20200908102904767.png)

## 深度学习技巧

### 深度学习的一般步骤

#### step1创建神经网络

1 define a set of function

2 goodness of function

3 pick the best function

-> neural network

#### step2检查结果是否符合要求

good results on training data？

if not : return to  step1 我们需要重新修改神经网络

if ok : good results on testing data?

​	if not : overfitting

![image-20200908143535036](.\deeplearning_pre\image-20200908143535036.png)

### 深度学习常用问题的解决方法

![image-20200908143949728](.\deeplearning_pre\image-20200908143949728.png)

#### 深度的增加，并不一定会得到更好的结果

随着神经网络深度的增加，在一些训练集上未必有很好的效果。

为什么会这样？按道理说，越深的网络，它应该能够向下兼容越浅的网络才对呀？你只需要前面几层与浅层神经网络一样，后面的几层什么事都不干就好了啊。

之所以出现这个问题，是因为激活函数sigmoid，它是一个数据维度会被不断拉窄的一个过程。

![image-20200908144144157](.\deeplearning_pre\image-20200908144144157.png)

在激活函数sigmoid的作用下，层数靠后的网络梯度较大，更新变化更为明显，但靠前的网络并非如此。它们往往具有较小的梯度，更新迭代较为缓慢。

这可能后面的网络已经到达局部最低点后，前面的网络仍然距离最低点较大的距离。

![image-20200908144307900](.\deeplearning_pre\image-20200908144307900.png)

从变化量的角度看，sigmoid函数会让数据的维度不断的变窄。

经过多层后，整个数据的维度将会变得非常低。

![image-20200908144515014](.\deeplearning_pre\image-20200908144515014.png)

#### 使用新的激活函数Relu

为了解决sigmoid函数带来的问题，使用relu函数是一个非常好的选择。

它的函数构成和具有的优点如下：

![image-20200908144824464](.\deeplearning_pre\image-20200908144824464.png)

Relu函数可以等价为一条线性函数。

但是这个线性函数不会让整个神经网络变得很weak。因为不同的输入，所得到的线性函数是不同的。所以其实可以看成是一个分段函数。

![image-20200908162551783](.\deeplearning_pre\image-20200908162551783.png)

#### Relu函数的变式

parametric Relu中，α自身也是一个可以被学习的参量（有点儿像又接了一个网络）。

![image-20200908163156330](.\deeplearning_pre\image-20200908163156330.png)

#### Maxout函数

maxout函数的作用方式如下：

![image-20200908163432106](.\deeplearning_pre\image-20200908163432106.png)

实际上，relu函数能拟合的模型，maxout函数也能够做到。如这个例子：

![image-20200908163748139](.\deeplearning_pre\image-20200908163748139.png)

但是maxout函数与relu函数相比，却可以拟合更为复杂的模型。

并且，随着合并的element越多，能拟合的分段函数的不同分段就越多。

![image-20200908163841369](.\deeplearning_pre\image-20200908163841369.png)

在maxout函数的作用下，一个输入，最终的输出就像是“经过选择”得到的一条线性函数。

实际上，不同的输入数据，实际经过的选择通路也是不同的。这样就算在局部是线性的，但这个线性函数是经过神经网络精挑细选得到的结果，整体上来看是一个高维的分段函数。

![image-20200908164222908](.\deeplearning_pre\image-20200908164222908.png)

### 优化方法

#### RMSProp

![image-20200908173748130](.\deeplearning_pre\image-20200908173748130.png)

![image-20200908175224060](.\deeplearning_pre\image-20200908175224060.png)

#### Momentum

**局部最小点**其实出现的概率并不大，因为在高维空间中，它必须是每一个维度同时都是局部最小点。

而多个维度同时是局部最小点的概率实在是太低了

![image-20200908182527348](.\deeplearning_pre\image-20200908182527348.png)

下面这个是传统的梯度下降的方式。

![image-20200908182620176](.\deeplearning_pre\image-20200908182620176.png)

这个是基于动量的方式，它会和前一次迭代的梯度进行矢量叠加。

![image-20200908182930207](.\deeplearning_pre\image-20200908182930207.png)

#### Adam

![image-20200910094503446](.\deeplearning_pre\image-20200910094503446.png)

### 深度学习技巧

#### 避免过拟合

![image-20200910095205541](.\deeplearning_pre\image-20200910095205541.png)

![image-20200910095317328](.\deeplearning_pre\image-20200910095317328.png)

Regularization 可以让 weight 降低，进而避免过拟合。

L2的参数，都会比较趋近于0

![image-20200910095528601](.\deeplearning_pre\image-20200910095528601.png)

L1的参数值，可能有特别大的，可能也有特别趋近于0的。

![image-20200910101903904](.\deeplearning_pre\image-20200910101903904.png)

#### Dropout

dropout 会随机“失活”掉一部分神经元

一个直觉化的解释是，似乎这样在训练的时候，受到其他神经元的影响会变小，从而让单个神经元的训练效果变的更好。

![image-20200910102657001](.\deeplearning_pre\image-20200910102657001.png)

当然，dropout中，训练时的权重与最终的权重有一个比值关系。

以失活率p = 1/2为例，最终使用的权重与训练时的权重相比，只有训练时的1/2的权重。

如果失活率p = 1/4。那么最终训练时候的权重，只有训练时候的3/4的权重。

![image-20200910102916486](.\deeplearning_pre\image-20200910102916486.png)

实际上dropout之所以可以让网络有效的解释应该是这个。

任何一个数据集，我们让它经过不同模型可以得到不同输出。不同输出的好坏自然是不同的。



如果我们将被dropout的那一层输出之前的网络，看成是不同的神经网络模型，这就意味着我们在使用多种模型进行评估。

![image-20200910103106336](.\deeplearning_pre\image-20200910103106336.png)

使用了dropout，就相当于有多个不同的神经网络在训练。

![image-20200910103235194](.\deeplearning_pre\image-20200910103235194.png)

![image-20200910103427845](.\deeplearning_pre\image-20200910103427845.png)

## 其他知识

### 语音辨识

![image-20200910152228976](.\deeplearning_pre\image-20200910152228976.png)

![image-20200910152943222](.\deeplearning_pre\image-20200910152943222.png)

在过往的语音辨识上，这些过程大量的依赖人工。

但现在，可以将一部分函数使用神经网络进行替代（如DCT，log）。

![image-20200910160842196](.\deeplearning_pre\image-20200910160842196.png)

那么可不可以全部使用神经网络呢？

![image-20200910161137863](.\deeplearning_pre\image-20200910161137863.png)

即便是某人使用了大量的神经网络进行训练了之后，使用纯神经网络替代后的表现结果，也与“信号与系统”的传统处理方法得到的表现结果相似。没有特别明显的优势。

### 拓展-为什么使用DeepLearning

![image-20200910162156631](.\deeplearning_pre\image-20200910162156631.png)